{
  "cells": [
    {
      "metadata": {
        "_uuid": "88e43f83b57eb37825e7a7b8ab3f4041686a32cf",
        "_cell_guid": "adea2cce-46da-4482-8ea0-c7128b560fed",
        "_kg_hide-output": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "from tensorflow.python.keras.applications import ResNet50\nfrom tensorflow.python.keras.models import Sequential\nfrom tensorflow.python.keras.layers import Dense, Flatten, GlobalAveragePooling2D\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport keras\nfrom keras.models import Sequential\nfrom keras.utils import np_utils\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.datasets import cifar10\nfrom keras import regularizers, optimizers\nimport numpy as np\nnum_classes = 10\nbaseMapNum = 32\nweight_decay = 1e-4\n#model\nmodel = Sequential()\nmodel.add(Conv2D(baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay), input_shape=(28,28,3)))\nmodel.add(Activation('relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\nmodel.add(Activation('relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.2))\n\nmodel.add(Conv2D(2*baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\nmodel.add(Activation('relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(2*baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\nmodel.add(Activation('relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.3))\n\nmodel.add(Flatten())\nmodel.add(Dense(num_classes, activation='softmax'))\n",
      "execution_count": 95,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f6bd8ba793ed55dad8cc016612f07f9ecd656007",
        "_cell_guid": "defc5fc9-af8e-4891-b41b-a38a1608bf09",
        "_kg_hide-output": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.summary()",
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_23 (Conv2D)           (None, 28, 28, 32)        896       \n_________________________________________________________________\nactivation_23 (Activation)   (None, 28, 28, 32)        0         \n_________________________________________________________________\nbatch_normalization_23 (Batc (None, 28, 28, 32)        128       \n_________________________________________________________________\nconv2d_24 (Conv2D)           (None, 28, 28, 32)        9248      \n_________________________________________________________________\nactivation_24 (Activation)   (None, 28, 28, 32)        0         \n_________________________________________________________________\nbatch_normalization_24 (Batc (None, 28, 28, 32)        128       \n_________________________________________________________________\nmax_pooling2d_12 (MaxPooling (None, 14, 14, 32)        0         \n_________________________________________________________________\ndropout_12 (Dropout)         (None, 14, 14, 32)        0         \n_________________________________________________________________\nconv2d_25 (Conv2D)           (None, 14, 14, 64)        18496     \n_________________________________________________________________\nactivation_25 (Activation)   (None, 14, 14, 64)        0         \n_________________________________________________________________\nbatch_normalization_25 (Batc (None, 14, 14, 64)        256       \n_________________________________________________________________\nconv2d_26 (Conv2D)           (None, 14, 14, 64)        36928     \n_________________________________________________________________\nactivation_26 (Activation)   (None, 14, 14, 64)        0         \n_________________________________________________________________\nbatch_normalization_26 (Batc (None, 14, 14, 64)        256       \n_________________________________________________________________\nmax_pooling2d_13 (MaxPooling (None, 7, 7, 64)          0         \n_________________________________________________________________\ndropout_13 (Dropout)         (None, 7, 7, 64)          0         \n_________________________________________________________________\nflatten_6 (Flatten)          (None, 3136)              0         \n_________________________________________________________________\ndense_6 (Dense)              (None, 10)                31370     \n=================================================================\nTotal params: 97,706\nTrainable params: 97,322\nNon-trainable params: 384\n_________________________________________________________________\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "48631321426fae639556c7ca0407927f2fae84d9",
        "_cell_guid": "07d8e9c7-043e-4b2a-a7c4-73791b283caf",
        "trusted": true
      },
      "cell_type": "code",
      "source": "#preparing train data\n\nfrom tensorflow.python.keras.applications.resnet50 import preprocess_input\nfrom sklearn.model_selection import train_test_split\n\nimport numpy as np\nimport cv2\nimport os\nimport pandas as pd\n\na = 28\nb = 28\n\ndef preprocess(img):\n\timg=255-np.array(img).astype(np.uint8)\n\timg = np.array(img).reshape(a,b,3)\n\timg= img.reshape(1,a*b*3)\n\timg = img/255\n\treturn img\n\nstr=\"\"\ni=0\nprint(os.listdir(\"../input/data-face/data/DATA\"))\nfinal = pd.DataFrame()\nfor fil in os.listdir(\"../input/data-face/data/DATA/train\"):\n            if fil.endswith(\".jpg\") : \n                str = os.path.join(fil)\n                st2 = \"../input/data-face/data/DATA/train\" +\"/\" +str\n                img = cv2.imread(st2)  \n                str =str.split('_')\n                str.pop()\n                str = [int(i) for i in str]\n                img2 = preprocess(img)\n                df = pd.DataFrame(img2)\n                df[a*b*3]= str[1]*5 + str[2]\n                final = final.append(df)\n                #print(i)\n                i+=1\n                if i//1000 ==15 :\n                    break\nprint(i)\nfinal.head()",
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": "['val', 'train']\n15000\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 97,
          "data": {
            "text/plain": "       0         1         2     ...       2350      2351  2352\n0  0.686275  0.631373  0.647059  ...   0.823529  0.839216     8\n0  0.709804  0.682353  0.529412  ...   0.886275  0.866667     0\n0  0.141176  0.164706  0.160784  ...   0.447059  0.411765     3\n0  0.913725  0.890196  0.862745  ...   0.913725  0.866667     9\n0  0.176471  0.105882  0.109804  ...   0.627451  0.141176     4\n\n[5 rows x 2353 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n      <th>12</th>\n      <th>13</th>\n      <th>14</th>\n      <th>15</th>\n      <th>16</th>\n      <th>17</th>\n      <th>18</th>\n      <th>19</th>\n      <th>20</th>\n      <th>21</th>\n      <th>22</th>\n      <th>23</th>\n      <th>24</th>\n      <th>25</th>\n      <th>26</th>\n      <th>27</th>\n      <th>28</th>\n      <th>29</th>\n      <th>30</th>\n      <th>31</th>\n      <th>32</th>\n      <th>33</th>\n      <th>34</th>\n      <th>35</th>\n      <th>36</th>\n      <th>37</th>\n      <th>38</th>\n      <th>39</th>\n      <th>...</th>\n      <th>2313</th>\n      <th>2314</th>\n      <th>2315</th>\n      <th>2316</th>\n      <th>2317</th>\n      <th>2318</th>\n      <th>2319</th>\n      <th>2320</th>\n      <th>2321</th>\n      <th>2322</th>\n      <th>2323</th>\n      <th>2324</th>\n      <th>2325</th>\n      <th>2326</th>\n      <th>2327</th>\n      <th>2328</th>\n      <th>2329</th>\n      <th>2330</th>\n      <th>2331</th>\n      <th>2332</th>\n      <th>2333</th>\n      <th>2334</th>\n      <th>2335</th>\n      <th>2336</th>\n      <th>2337</th>\n      <th>2338</th>\n      <th>2339</th>\n      <th>2340</th>\n      <th>2341</th>\n      <th>2342</th>\n      <th>2343</th>\n      <th>2344</th>\n      <th>2345</th>\n      <th>2346</th>\n      <th>2347</th>\n      <th>2348</th>\n      <th>2349</th>\n      <th>2350</th>\n      <th>2351</th>\n      <th>2352</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.686275</td>\n      <td>0.631373</td>\n      <td>0.647059</td>\n      <td>0.682353</td>\n      <td>0.627451</td>\n      <td>0.643137</td>\n      <td>0.725490</td>\n      <td>0.670588</td>\n      <td>0.686275</td>\n      <td>0.772549</td>\n      <td>0.717647</td>\n      <td>0.733333</td>\n      <td>0.694118</td>\n      <td>0.650980</td>\n      <td>0.658824</td>\n      <td>0.529412</td>\n      <td>0.486275</td>\n      <td>0.494118</td>\n      <td>0.407843</td>\n      <td>0.364706</td>\n      <td>0.372549</td>\n      <td>0.372549</td>\n      <td>0.329412</td>\n      <td>0.337255</td>\n      <td>0.231373</td>\n      <td>0.200000</td>\n      <td>0.200000</td>\n      <td>0.180392</td>\n      <td>0.149020</td>\n      <td>0.149020</td>\n      <td>0.125490</td>\n      <td>0.105882</td>\n      <td>0.094118</td>\n      <td>0.113725</td>\n      <td>0.094118</td>\n      <td>0.082353</td>\n      <td>0.117647</td>\n      <td>0.098039</td>\n      <td>0.086275</td>\n      <td>0.117647</td>\n      <td>...</td>\n      <td>0.290196</td>\n      <td>0.294118</td>\n      <td>0.278431</td>\n      <td>0.356863</td>\n      <td>0.360784</td>\n      <td>0.345098</td>\n      <td>0.525490</td>\n      <td>0.529412</td>\n      <td>0.521569</td>\n      <td>0.650980</td>\n      <td>0.650980</td>\n      <td>0.650980</td>\n      <td>0.705882</td>\n      <td>0.705882</td>\n      <td>0.705882</td>\n      <td>0.784314</td>\n      <td>0.784314</td>\n      <td>0.784314</td>\n      <td>0.847059</td>\n      <td>0.847059</td>\n      <td>0.847059</td>\n      <td>0.854902</td>\n      <td>0.850980</td>\n      <td>0.858824</td>\n      <td>0.854902</td>\n      <td>0.850980</td>\n      <td>0.858824</td>\n      <td>0.827451</td>\n      <td>0.823529</td>\n      <td>0.831373</td>\n      <td>0.827451</td>\n      <td>0.823529</td>\n      <td>0.831373</td>\n      <td>0.827451</td>\n      <td>0.823529</td>\n      <td>0.839216</td>\n      <td>0.827451</td>\n      <td>0.823529</td>\n      <td>0.839216</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>0.709804</td>\n      <td>0.682353</td>\n      <td>0.529412</td>\n      <td>0.631373</td>\n      <td>0.603922</td>\n      <td>0.450980</td>\n      <td>0.403922</td>\n      <td>0.368627</td>\n      <td>0.219608</td>\n      <td>0.345098</td>\n      <td>0.309804</td>\n      <td>0.160784</td>\n      <td>0.329412</td>\n      <td>0.294118</td>\n      <td>0.145098</td>\n      <td>0.290196</td>\n      <td>0.254902</td>\n      <td>0.105882</td>\n      <td>0.333333</td>\n      <td>0.290196</td>\n      <td>0.141176</td>\n      <td>0.243137</td>\n      <td>0.200000</td>\n      <td>0.050980</td>\n      <td>0.274510</td>\n      <td>0.235294</td>\n      <td>0.078431</td>\n      <td>0.286275</td>\n      <td>0.247059</td>\n      <td>0.090196</td>\n      <td>0.278431</td>\n      <td>0.231373</td>\n      <td>0.074510</td>\n      <td>0.247059</td>\n      <td>0.200000</td>\n      <td>0.043137</td>\n      <td>0.266667</td>\n      <td>0.219608</td>\n      <td>0.062745</td>\n      <td>0.325490</td>\n      <td>...</td>\n      <td>0.521569</td>\n      <td>0.474510</td>\n      <td>0.333333</td>\n      <td>0.584314</td>\n      <td>0.521569</td>\n      <td>0.376471</td>\n      <td>0.572549</td>\n      <td>0.513725</td>\n      <td>0.384314</td>\n      <td>0.352941</td>\n      <td>0.317647</td>\n      <td>0.211765</td>\n      <td>0.188235</td>\n      <td>0.172549</td>\n      <td>0.098039</td>\n      <td>0.184314</td>\n      <td>0.188235</td>\n      <td>0.149020</td>\n      <td>0.231373</td>\n      <td>0.247059</td>\n      <td>0.227451</td>\n      <td>0.450980</td>\n      <td>0.478431</td>\n      <td>0.466667</td>\n      <td>0.788235</td>\n      <td>0.807843</td>\n      <td>0.803922</td>\n      <td>0.780392</td>\n      <td>0.796078</td>\n      <td>0.776471</td>\n      <td>0.835294</td>\n      <td>0.831373</td>\n      <td>0.815686</td>\n      <td>0.878431</td>\n      <td>0.866667</td>\n      <td>0.847059</td>\n      <td>0.901961</td>\n      <td>0.886275</td>\n      <td>0.866667</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>0.141176</td>\n      <td>0.164706</td>\n      <td>0.160784</td>\n      <td>0.800000</td>\n      <td>0.807843</td>\n      <td>0.784314</td>\n      <td>0.803922</td>\n      <td>0.776471</td>\n      <td>0.717647</td>\n      <td>0.560784</td>\n      <td>0.494118</td>\n      <td>0.392157</td>\n      <td>0.462745</td>\n      <td>0.349020</td>\n      <td>0.203922</td>\n      <td>0.513725</td>\n      <td>0.360784</td>\n      <td>0.184314</td>\n      <td>0.458824</td>\n      <td>0.274510</td>\n      <td>0.074510</td>\n      <td>0.435294</td>\n      <td>0.239216</td>\n      <td>0.031373</td>\n      <td>0.411765</td>\n      <td>0.223529</td>\n      <td>0.000000</td>\n      <td>0.396078</td>\n      <td>0.215686</td>\n      <td>0.000000</td>\n      <td>0.380392</td>\n      <td>0.211765</td>\n      <td>0.000000</td>\n      <td>0.384314</td>\n      <td>0.235294</td>\n      <td>0.007843</td>\n      <td>0.415686</td>\n      <td>0.278431</td>\n      <td>0.031373</td>\n      <td>0.431373</td>\n      <td>...</td>\n      <td>0.482353</td>\n      <td>0.415686</td>\n      <td>0.247059</td>\n      <td>0.529412</td>\n      <td>0.478431</td>\n      <td>0.305882</td>\n      <td>0.572549</td>\n      <td>0.517647</td>\n      <td>0.352941</td>\n      <td>0.658824</td>\n      <td>0.584314</td>\n      <td>0.435294</td>\n      <td>0.756863</td>\n      <td>0.662745</td>\n      <td>0.521569</td>\n      <td>0.807843</td>\n      <td>0.713725</td>\n      <td>0.572549</td>\n      <td>0.788235</td>\n      <td>0.690196</td>\n      <td>0.564706</td>\n      <td>0.678431</td>\n      <td>0.592157</td>\n      <td>0.482353</td>\n      <td>0.568627</td>\n      <td>0.501961</td>\n      <td>0.400000</td>\n      <td>0.623529</td>\n      <td>0.580392</td>\n      <td>0.501961</td>\n      <td>0.552941</td>\n      <td>0.525490</td>\n      <td>0.466667</td>\n      <td>0.474510</td>\n      <td>0.474510</td>\n      <td>0.427451</td>\n      <td>0.443137</td>\n      <td>0.447059</td>\n      <td>0.411765</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>0.913725</td>\n      <td>0.890196</td>\n      <td>0.862745</td>\n      <td>0.862745</td>\n      <td>0.835294</td>\n      <td>0.800000</td>\n      <td>0.858824</td>\n      <td>0.835294</td>\n      <td>0.792157</td>\n      <td>0.933333</td>\n      <td>0.898039</td>\n      <td>0.843137</td>\n      <td>0.980392</td>\n      <td>0.941176</td>\n      <td>0.870588</td>\n      <td>0.992157</td>\n      <td>0.941176</td>\n      <td>0.854902</td>\n      <td>1.000000</td>\n      <td>0.984314</td>\n      <td>0.890196</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.921569</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.917647</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.921569</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.909804</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.917647</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.933333</td>\n      <td>1.000000</td>\n      <td>...</td>\n      <td>0.909804</td>\n      <td>0.847059</td>\n      <td>0.823529</td>\n      <td>0.937255</td>\n      <td>0.905882</td>\n      <td>0.839216</td>\n      <td>0.976471</td>\n      <td>0.952941</td>\n      <td>0.878431</td>\n      <td>0.996078</td>\n      <td>0.984314</td>\n      <td>0.925490</td>\n      <td>1.000000</td>\n      <td>0.980392</td>\n      <td>0.952941</td>\n      <td>0.988235</td>\n      <td>0.976471</td>\n      <td>0.960784</td>\n      <td>0.984314</td>\n      <td>0.964706</td>\n      <td>0.968627</td>\n      <td>0.964706</td>\n      <td>0.964706</td>\n      <td>0.964706</td>\n      <td>0.952941</td>\n      <td>0.956863</td>\n      <td>0.949020</td>\n      <td>0.925490</td>\n      <td>0.941176</td>\n      <td>0.917647</td>\n      <td>0.909804</td>\n      <td>0.929412</td>\n      <td>0.894118</td>\n      <td>0.901961</td>\n      <td>0.917647</td>\n      <td>0.870588</td>\n      <td>0.898039</td>\n      <td>0.913725</td>\n      <td>0.866667</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>0.176471</td>\n      <td>0.105882</td>\n      <td>0.109804</td>\n      <td>0.145098</td>\n      <td>0.070588</td>\n      <td>0.050980</td>\n      <td>0.807843</td>\n      <td>0.733333</td>\n      <td>0.674510</td>\n      <td>1.000000</td>\n      <td>0.972549</td>\n      <td>0.866667</td>\n      <td>0.764706</td>\n      <td>0.686275</td>\n      <td>0.525490</td>\n      <td>0.670588</td>\n      <td>0.572549</td>\n      <td>0.376471</td>\n      <td>0.635294</td>\n      <td>0.529412</td>\n      <td>0.305882</td>\n      <td>0.611765</td>\n      <td>0.490196</td>\n      <td>0.250980</td>\n      <td>0.568627</td>\n      <td>0.435294</td>\n      <td>0.200000</td>\n      <td>0.600000</td>\n      <td>0.450980</td>\n      <td>0.223529</td>\n      <td>0.619608</td>\n      <td>0.470588</td>\n      <td>0.243137</td>\n      <td>0.635294</td>\n      <td>0.474510</td>\n      <td>0.258824</td>\n      <td>0.623529</td>\n      <td>0.470588</td>\n      <td>0.254902</td>\n      <td>0.627451</td>\n      <td>...</td>\n      <td>0.760784</td>\n      <td>0.666667</td>\n      <td>0.462745</td>\n      <td>0.760784</td>\n      <td>0.698039</td>\n      <td>0.466667</td>\n      <td>0.788235</td>\n      <td>0.741176</td>\n      <td>0.513725</td>\n      <td>0.858824</td>\n      <td>0.792157</td>\n      <td>0.592157</td>\n      <td>0.909804</td>\n      <td>0.835294</td>\n      <td>0.658824</td>\n      <td>0.933333</td>\n      <td>0.854902</td>\n      <td>0.694118</td>\n      <td>0.925490</td>\n      <td>0.850980</td>\n      <td>0.682353</td>\n      <td>0.909804</td>\n      <td>0.843137</td>\n      <td>0.623529</td>\n      <td>0.882353</td>\n      <td>0.831373</td>\n      <td>0.564706</td>\n      <td>0.933333</td>\n      <td>0.901961</td>\n      <td>0.552941</td>\n      <td>0.941176</td>\n      <td>0.929412</td>\n      <td>0.521569</td>\n      <td>0.870588</td>\n      <td>0.870588</td>\n      <td>0.407843</td>\n      <td>0.619608</td>\n      <td>0.627451</td>\n      <td>0.141176</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "66018cf5862b475c0a6bef3f4a60c756215d80dd"
      },
      "cell_type": "code",
      "source": "#preparing test data\n\nfin = pd.DataFrame()\nfor fil in os.listdir(\"../input/data-face/data/DATA/val\"):\n            if fil.endswith(\".jpg\") : \n                str = os.path.join(fil)\n                st2 = \"../input/data-face/data/DATA/val\" +\"/\" +str\n                img = cv2.imread(st2)  \n                str =str.split('_')\n                str.pop()\n                str = [int(i) for i in str]\n                img2 = preprocess(img)\n                df = pd.DataFrame(img2)\n                df[a*b*3]= str[1]*5 + str[2]\n                fin = fin.append(df)\n                #print(i)\n                i+=1\nfin.head()",
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": "24778\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 98,
          "data": {
            "text/plain": "       0         1         2     ...       2350      2351  2352\n0  0.686275  0.631373  0.647059  ...   0.823529  0.839216     8\n0  0.709804  0.682353  0.529412  ...   0.886275  0.866667     0\n0  0.913725  0.890196  0.862745  ...   0.913725  0.866667     9\n0  0.176471  0.105882  0.109804  ...   0.627451  0.141176     4\n0  0.764706  0.831373  0.937255  ...   0.431373  0.392157     6\n\n[5 rows x 2353 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n      <th>12</th>\n      <th>13</th>\n      <th>14</th>\n      <th>15</th>\n      <th>16</th>\n      <th>17</th>\n      <th>18</th>\n      <th>19</th>\n      <th>20</th>\n      <th>21</th>\n      <th>22</th>\n      <th>23</th>\n      <th>24</th>\n      <th>25</th>\n      <th>26</th>\n      <th>27</th>\n      <th>28</th>\n      <th>29</th>\n      <th>30</th>\n      <th>31</th>\n      <th>32</th>\n      <th>33</th>\n      <th>34</th>\n      <th>35</th>\n      <th>36</th>\n      <th>37</th>\n      <th>38</th>\n      <th>39</th>\n      <th>...</th>\n      <th>2313</th>\n      <th>2314</th>\n      <th>2315</th>\n      <th>2316</th>\n      <th>2317</th>\n      <th>2318</th>\n      <th>2319</th>\n      <th>2320</th>\n      <th>2321</th>\n      <th>2322</th>\n      <th>2323</th>\n      <th>2324</th>\n      <th>2325</th>\n      <th>2326</th>\n      <th>2327</th>\n      <th>2328</th>\n      <th>2329</th>\n      <th>2330</th>\n      <th>2331</th>\n      <th>2332</th>\n      <th>2333</th>\n      <th>2334</th>\n      <th>2335</th>\n      <th>2336</th>\n      <th>2337</th>\n      <th>2338</th>\n      <th>2339</th>\n      <th>2340</th>\n      <th>2341</th>\n      <th>2342</th>\n      <th>2343</th>\n      <th>2344</th>\n      <th>2345</th>\n      <th>2346</th>\n      <th>2347</th>\n      <th>2348</th>\n      <th>2349</th>\n      <th>2350</th>\n      <th>2351</th>\n      <th>2352</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.686275</td>\n      <td>0.631373</td>\n      <td>0.647059</td>\n      <td>0.682353</td>\n      <td>0.627451</td>\n      <td>0.643137</td>\n      <td>0.725490</td>\n      <td>0.670588</td>\n      <td>0.686275</td>\n      <td>0.772549</td>\n      <td>0.717647</td>\n      <td>0.733333</td>\n      <td>0.694118</td>\n      <td>0.650980</td>\n      <td>0.658824</td>\n      <td>0.529412</td>\n      <td>0.486275</td>\n      <td>0.494118</td>\n      <td>0.407843</td>\n      <td>0.364706</td>\n      <td>0.372549</td>\n      <td>0.372549</td>\n      <td>0.329412</td>\n      <td>0.337255</td>\n      <td>0.231373</td>\n      <td>0.200000</td>\n      <td>0.200000</td>\n      <td>0.180392</td>\n      <td>0.149020</td>\n      <td>0.149020</td>\n      <td>0.125490</td>\n      <td>0.105882</td>\n      <td>0.094118</td>\n      <td>0.113725</td>\n      <td>0.094118</td>\n      <td>0.082353</td>\n      <td>0.117647</td>\n      <td>0.098039</td>\n      <td>0.086275</td>\n      <td>0.117647</td>\n      <td>...</td>\n      <td>0.290196</td>\n      <td>0.294118</td>\n      <td>0.278431</td>\n      <td>0.356863</td>\n      <td>0.360784</td>\n      <td>0.345098</td>\n      <td>0.525490</td>\n      <td>0.529412</td>\n      <td>0.521569</td>\n      <td>0.650980</td>\n      <td>0.650980</td>\n      <td>0.650980</td>\n      <td>0.705882</td>\n      <td>0.705882</td>\n      <td>0.705882</td>\n      <td>0.784314</td>\n      <td>0.784314</td>\n      <td>0.784314</td>\n      <td>0.847059</td>\n      <td>0.847059</td>\n      <td>0.847059</td>\n      <td>0.854902</td>\n      <td>0.850980</td>\n      <td>0.858824</td>\n      <td>0.854902</td>\n      <td>0.850980</td>\n      <td>0.858824</td>\n      <td>0.827451</td>\n      <td>0.823529</td>\n      <td>0.831373</td>\n      <td>0.827451</td>\n      <td>0.823529</td>\n      <td>0.831373</td>\n      <td>0.827451</td>\n      <td>0.823529</td>\n      <td>0.839216</td>\n      <td>0.827451</td>\n      <td>0.823529</td>\n      <td>0.839216</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>0.709804</td>\n      <td>0.682353</td>\n      <td>0.529412</td>\n      <td>0.631373</td>\n      <td>0.603922</td>\n      <td>0.450980</td>\n      <td>0.403922</td>\n      <td>0.368627</td>\n      <td>0.219608</td>\n      <td>0.345098</td>\n      <td>0.309804</td>\n      <td>0.160784</td>\n      <td>0.329412</td>\n      <td>0.294118</td>\n      <td>0.145098</td>\n      <td>0.290196</td>\n      <td>0.254902</td>\n      <td>0.105882</td>\n      <td>0.333333</td>\n      <td>0.290196</td>\n      <td>0.141176</td>\n      <td>0.243137</td>\n      <td>0.200000</td>\n      <td>0.050980</td>\n      <td>0.274510</td>\n      <td>0.235294</td>\n      <td>0.078431</td>\n      <td>0.286275</td>\n      <td>0.247059</td>\n      <td>0.090196</td>\n      <td>0.278431</td>\n      <td>0.231373</td>\n      <td>0.074510</td>\n      <td>0.247059</td>\n      <td>0.200000</td>\n      <td>0.043137</td>\n      <td>0.266667</td>\n      <td>0.219608</td>\n      <td>0.062745</td>\n      <td>0.325490</td>\n      <td>...</td>\n      <td>0.521569</td>\n      <td>0.474510</td>\n      <td>0.333333</td>\n      <td>0.584314</td>\n      <td>0.521569</td>\n      <td>0.376471</td>\n      <td>0.572549</td>\n      <td>0.513725</td>\n      <td>0.384314</td>\n      <td>0.352941</td>\n      <td>0.317647</td>\n      <td>0.211765</td>\n      <td>0.188235</td>\n      <td>0.172549</td>\n      <td>0.098039</td>\n      <td>0.184314</td>\n      <td>0.188235</td>\n      <td>0.149020</td>\n      <td>0.231373</td>\n      <td>0.247059</td>\n      <td>0.227451</td>\n      <td>0.450980</td>\n      <td>0.478431</td>\n      <td>0.466667</td>\n      <td>0.788235</td>\n      <td>0.807843</td>\n      <td>0.803922</td>\n      <td>0.780392</td>\n      <td>0.796078</td>\n      <td>0.776471</td>\n      <td>0.835294</td>\n      <td>0.831373</td>\n      <td>0.815686</td>\n      <td>0.878431</td>\n      <td>0.866667</td>\n      <td>0.847059</td>\n      <td>0.901961</td>\n      <td>0.886275</td>\n      <td>0.866667</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>0.913725</td>\n      <td>0.890196</td>\n      <td>0.862745</td>\n      <td>0.862745</td>\n      <td>0.835294</td>\n      <td>0.800000</td>\n      <td>0.858824</td>\n      <td>0.835294</td>\n      <td>0.792157</td>\n      <td>0.933333</td>\n      <td>0.898039</td>\n      <td>0.843137</td>\n      <td>0.980392</td>\n      <td>0.941176</td>\n      <td>0.870588</td>\n      <td>0.992157</td>\n      <td>0.941176</td>\n      <td>0.854902</td>\n      <td>1.000000</td>\n      <td>0.984314</td>\n      <td>0.890196</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.921569</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.917647</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.921569</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.909804</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.917647</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.933333</td>\n      <td>1.000000</td>\n      <td>...</td>\n      <td>0.909804</td>\n      <td>0.847059</td>\n      <td>0.823529</td>\n      <td>0.937255</td>\n      <td>0.905882</td>\n      <td>0.839216</td>\n      <td>0.976471</td>\n      <td>0.952941</td>\n      <td>0.878431</td>\n      <td>0.996078</td>\n      <td>0.984314</td>\n      <td>0.925490</td>\n      <td>1.000000</td>\n      <td>0.980392</td>\n      <td>0.952941</td>\n      <td>0.988235</td>\n      <td>0.976471</td>\n      <td>0.960784</td>\n      <td>0.984314</td>\n      <td>0.964706</td>\n      <td>0.968627</td>\n      <td>0.964706</td>\n      <td>0.964706</td>\n      <td>0.964706</td>\n      <td>0.952941</td>\n      <td>0.956863</td>\n      <td>0.949020</td>\n      <td>0.925490</td>\n      <td>0.941176</td>\n      <td>0.917647</td>\n      <td>0.909804</td>\n      <td>0.929412</td>\n      <td>0.894118</td>\n      <td>0.901961</td>\n      <td>0.917647</td>\n      <td>0.870588</td>\n      <td>0.898039</td>\n      <td>0.913725</td>\n      <td>0.866667</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>0.176471</td>\n      <td>0.105882</td>\n      <td>0.109804</td>\n      <td>0.145098</td>\n      <td>0.070588</td>\n      <td>0.050980</td>\n      <td>0.807843</td>\n      <td>0.733333</td>\n      <td>0.674510</td>\n      <td>1.000000</td>\n      <td>0.972549</td>\n      <td>0.866667</td>\n      <td>0.764706</td>\n      <td>0.686275</td>\n      <td>0.525490</td>\n      <td>0.670588</td>\n      <td>0.572549</td>\n      <td>0.376471</td>\n      <td>0.635294</td>\n      <td>0.529412</td>\n      <td>0.305882</td>\n      <td>0.611765</td>\n      <td>0.490196</td>\n      <td>0.250980</td>\n      <td>0.568627</td>\n      <td>0.435294</td>\n      <td>0.200000</td>\n      <td>0.600000</td>\n      <td>0.450980</td>\n      <td>0.223529</td>\n      <td>0.619608</td>\n      <td>0.470588</td>\n      <td>0.243137</td>\n      <td>0.635294</td>\n      <td>0.474510</td>\n      <td>0.258824</td>\n      <td>0.623529</td>\n      <td>0.470588</td>\n      <td>0.254902</td>\n      <td>0.627451</td>\n      <td>...</td>\n      <td>0.760784</td>\n      <td>0.666667</td>\n      <td>0.462745</td>\n      <td>0.760784</td>\n      <td>0.698039</td>\n      <td>0.466667</td>\n      <td>0.788235</td>\n      <td>0.741176</td>\n      <td>0.513725</td>\n      <td>0.858824</td>\n      <td>0.792157</td>\n      <td>0.592157</td>\n      <td>0.909804</td>\n      <td>0.835294</td>\n      <td>0.658824</td>\n      <td>0.933333</td>\n      <td>0.854902</td>\n      <td>0.694118</td>\n      <td>0.925490</td>\n      <td>0.850980</td>\n      <td>0.682353</td>\n      <td>0.909804</td>\n      <td>0.843137</td>\n      <td>0.623529</td>\n      <td>0.882353</td>\n      <td>0.831373</td>\n      <td>0.564706</td>\n      <td>0.933333</td>\n      <td>0.901961</td>\n      <td>0.552941</td>\n      <td>0.941176</td>\n      <td>0.929412</td>\n      <td>0.521569</td>\n      <td>0.870588</td>\n      <td>0.870588</td>\n      <td>0.407843</td>\n      <td>0.619608</td>\n      <td>0.627451</td>\n      <td>0.141176</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>0.764706</td>\n      <td>0.831373</td>\n      <td>0.937255</td>\n      <td>0.800000</td>\n      <td>0.831373</td>\n      <td>0.921569</td>\n      <td>0.737255</td>\n      <td>0.725490</td>\n      <td>0.760784</td>\n      <td>0.658824</td>\n      <td>0.584314</td>\n      <td>0.564706</td>\n      <td>0.721569</td>\n      <td>0.592157</td>\n      <td>0.513725</td>\n      <td>0.815686</td>\n      <td>0.650980</td>\n      <td>0.513725</td>\n      <td>0.772549</td>\n      <td>0.600000</td>\n      <td>0.415686</td>\n      <td>0.705882</td>\n      <td>0.533333</td>\n      <td>0.325490</td>\n      <td>0.658824</td>\n      <td>0.525490</td>\n      <td>0.294118</td>\n      <td>0.462745</td>\n      <td>0.349020</td>\n      <td>0.125490</td>\n      <td>0.356863</td>\n      <td>0.278431</td>\n      <td>0.054902</td>\n      <td>0.333333</td>\n      <td>0.262745</td>\n      <td>0.054902</td>\n      <td>0.266667</td>\n      <td>0.203922</td>\n      <td>0.000000</td>\n      <td>0.290196</td>\n      <td>...</td>\n      <td>1.000000</td>\n      <td>0.905882</td>\n      <td>0.796078</td>\n      <td>1.000000</td>\n      <td>0.921569</td>\n      <td>0.827451</td>\n      <td>1.000000</td>\n      <td>0.913725</td>\n      <td>0.835294</td>\n      <td>1.000000</td>\n      <td>0.921569</td>\n      <td>0.839216</td>\n      <td>1.000000</td>\n      <td>0.941176</td>\n      <td>0.874510</td>\n      <td>0.984314</td>\n      <td>0.913725</td>\n      <td>0.847059</td>\n      <td>0.945098</td>\n      <td>0.878431</td>\n      <td>0.827451</td>\n      <td>0.956863</td>\n      <td>0.905882</td>\n      <td>0.850980</td>\n      <td>0.968627</td>\n      <td>0.913725</td>\n      <td>0.866667</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.960784</td>\n      <td>0.776471</td>\n      <td>0.729412</td>\n      <td>0.690196</td>\n      <td>0.462745</td>\n      <td>0.415686</td>\n      <td>0.376471</td>\n      <td>0.478431</td>\n      <td>0.431373</td>\n      <td>0.392157</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c92a3a8762ed4256c6e10ee86cc03a7b0143c71b"
      },
      "cell_type": "code",
      "source": "#Train/ Fit the model on train data\nY = final[2352]\nX = final.drop(columns=[2352])\nY = tf.keras.utils.to_categorical(Y,10)\nX = np.array(X)\nX = X.reshape(15000,28,28,3)\nY_v = fin[2352]\nX_v = fin.drop(columns=[2352])\nY_v = tf.keras.utils.to_categorical(Y_v,10)\nX_v = np.array(X_v)\nX_v = X_v.reshape(9778,28,28,3)\nmodel.fit(X,Y,epochs =50)",
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Epoch 1/15\n15000/15000 [==============================] - 11s 726us/step - loss: 0.3220 - acc: 0.9259\nEpoch 2/15\n15000/15000 [==============================] - 11s 727us/step - loss: 0.3319 - acc: 0.9234\nEpoch 3/15\n15000/15000 [==============================] - 11s 720us/step - loss: 0.3282 - acc: 0.9223\nEpoch 4/15\n15000/15000 [==============================] - 11s 726us/step - loss: 0.3261 - acc: 0.9235\nEpoch 5/15\n15000/15000 [==============================] - 11s 725us/step - loss: 0.3249 - acc: 0.9248\nEpoch 6/15\n15000/15000 [==============================] - 11s 725us/step - loss: 0.3238 - acc: 0.9243\nEpoch 7/15\n15000/15000 [==============================] - 11s 724us/step - loss: 0.3239 - acc: 0.9267\nEpoch 8/15\n15000/15000 [==============================] - 11s 734us/step - loss: 0.3234 - acc: 0.9270\nEpoch 9/15\n15000/15000 [==============================] - 11s 727us/step - loss: 0.3306 - acc: 0.9235\nEpoch 10/15\n15000/15000 [==============================] - 11s 727us/step - loss: 0.3210 - acc: 0.9259\nEpoch 11/15\n15000/15000 [==============================] - 11s 725us/step - loss: 0.3141 - acc: 0.9323\nEpoch 12/15\n15000/15000 [==============================] - 11s 722us/step - loss: 0.3257 - acc: 0.9262\nEpoch 13/15\n15000/15000 [==============================] - 11s 722us/step - loss: 0.3277 - acc: 0.9253\nEpoch 14/15\n15000/15000 [==============================] - 11s 726us/step - loss: 0.3185 - acc: 0.9275\nEpoch 15/15\n15000/15000 [==============================] - 11s 728us/step - loss: 0.3175 - acc: 0.9292\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 128,
          "data": {
            "text/plain": "<keras.callbacks.History at 0x7ff4f86005f8>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8fbdbbde64178c957fc337564a1dea1e5e11b761"
      },
      "cell_type": "code",
      "source": "#Evaluating on test data\nmodel.save('weights.h5')\npred= model.evaluate( X_v[:7150],Y_v[:7150])\nprint(\"On test data:\")\nprint('Loss: ',pred[0])\nacc = pred[1]*100\nprint('Accuracy: ',acc,\"%\")",
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": "7150/7150 [==============================] - 2s 300us/step\nOn test data:\nLoss:  0.384139076346284\nAccuracy:  94.19580419413694 %\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "17d466d1854513fb8a55597bafaab47940c64734"
      },
      "cell_type": "code",
      "source": "#Notes\nprint('Has scope for Improvements')",
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Has scope for Improvements\n",
          "name": "stdout"
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}
