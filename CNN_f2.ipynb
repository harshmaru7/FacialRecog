{
  "cells": [
    {
      "metadata": {
        "_cell_guid": "adea2cce-46da-4482-8ea0-c7128b560fed",
        "_kg_hide-output": true,
        "_uuid": "88e43f83b57eb37825e7a7b8ab3f4041686a32cf",
        "trusted": true
      },
      "cell_type": "code",
      "source": "from tensorflow.python.keras.applications import ResNet50\nfrom tensorflow.python.keras.models import Sequential\nfrom tensorflow.python.keras.layers import Dense, Flatten, GlobalAveragePooling2D\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport keras\nfrom keras.models import Sequential\nfrom keras.utils import np_utils\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.datasets import cifar10\nfrom keras import regularizers, optimizers\nimport numpy as np\nnum_classes = 10\nbaseMapNum = 32\nweight_decay = 1e-4\n#model\nmodel = Sequential()\nmodel.add(Conv2D(baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay), input_shape=(28,28,3)))\nmodel.add(Activation('relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\nmodel.add(Activation('relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.1))\n\nmodel.add(Conv2D(2*baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\nmodel.add(Activation('relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(2*baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\nmodel.add(Activation('relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.2))\n\nmodel.add(Flatten())\nmodel.add(Dense(5*num_classes, activation='relu'))\nmodel.add(Dense(num_classes, activation='softmax'))\n",
      "execution_count": 30,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "defc5fc9-af8e-4891-b41b-a38a1608bf09",
        "_kg_hide-output": true,
        "_uuid": "f6bd8ba793ed55dad8cc016612f07f9ecd656007",
        "trusted": true
      },
      "cell_type": "code",
      "source": "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.summary()",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_18 (Conv2D)           (None, 28, 28, 32)        896       \n_________________________________________________________________\nactivation_17 (Activation)   (None, 28, 28, 32)        0         \n_________________________________________________________________\nbatch_normalization_17 (Batc (None, 28, 28, 32)        128       \n_________________________________________________________________\nconv2d_19 (Conv2D)           (None, 28, 28, 32)        9248      \n_________________________________________________________________\nactivation_18 (Activation)   (None, 28, 28, 32)        0         \n_________________________________________________________________\nbatch_normalization_18 (Batc (None, 28, 28, 32)        128       \n_________________________________________________________________\nmax_pooling2d_9 (MaxPooling2 (None, 14, 14, 32)        0         \n_________________________________________________________________\ndropout_9 (Dropout)          (None, 14, 14, 32)        0         \n_________________________________________________________________\nconv2d_20 (Conv2D)           (None, 14, 14, 64)        18496     \n_________________________________________________________________\nactivation_19 (Activation)   (None, 14, 14, 64)        0         \n_________________________________________________________________\nbatch_normalization_19 (Batc (None, 14, 14, 64)        256       \n_________________________________________________________________\nconv2d_21 (Conv2D)           (None, 14, 14, 64)        36928     \n_________________________________________________________________\nactivation_20 (Activation)   (None, 14, 14, 64)        0         \n_________________________________________________________________\nbatch_normalization_20 (Batc (None, 14, 14, 64)        256       \n_________________________________________________________________\nmax_pooling2d_10 (MaxPooling (None, 7, 7, 64)          0         \n_________________________________________________________________\ndropout_10 (Dropout)         (None, 7, 7, 64)          0         \n_________________________________________________________________\nflatten_5 (Flatten)          (None, 3136)              0         \n_________________________________________________________________\ndense_6 (Dense)              (None, 50)                156850    \n_________________________________________________________________\ndense_7 (Dense)              (None, 10)                510       \n=================================================================\nTotal params: 223,696\nTrainable params: 223,312\nNon-trainable params: 384\n_________________________________________________________________\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "07d8e9c7-043e-4b2a-a7c4-73791b283caf",
        "_uuid": "48631321426fae639556c7ca0407927f2fae84d9",
        "trusted": true
      },
      "cell_type": "code",
      "source": "#preparing train data\nfinal = pd.read_csv('../input/train.csv')\nfinal = final.drop(columns=[\"Unnamed: 0\"])\nfinal.head()",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 32,
          "data": {
            "text/plain": "          0         1         2  ...       2350      2351  2352\n0  0.000000  0.003922  0.000000  ...   0.537255  0.415686     0\n1  0.215686  0.219608  0.235294  ...   1.000000  0.956863     2\n2  0.443137  0.466667  0.423529  ...   0.901961  0.949020     0\n3  0.784314  0.764706  0.729412  ...   0.980392  0.996078     0\n4  0.788235  0.745098  0.756863  ...   0.874510  0.843137     6\n\n[5 rows x 2353 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n      <th>12</th>\n      <th>13</th>\n      <th>14</th>\n      <th>15</th>\n      <th>16</th>\n      <th>17</th>\n      <th>18</th>\n      <th>19</th>\n      <th>20</th>\n      <th>21</th>\n      <th>22</th>\n      <th>23</th>\n      <th>24</th>\n      <th>25</th>\n      <th>26</th>\n      <th>27</th>\n      <th>28</th>\n      <th>29</th>\n      <th>30</th>\n      <th>31</th>\n      <th>32</th>\n      <th>33</th>\n      <th>34</th>\n      <th>35</th>\n      <th>36</th>\n      <th>37</th>\n      <th>38</th>\n      <th>39</th>\n      <th>...</th>\n      <th>2313</th>\n      <th>2314</th>\n      <th>2315</th>\n      <th>2316</th>\n      <th>2317</th>\n      <th>2318</th>\n      <th>2319</th>\n      <th>2320</th>\n      <th>2321</th>\n      <th>2322</th>\n      <th>2323</th>\n      <th>2324</th>\n      <th>2325</th>\n      <th>2326</th>\n      <th>2327</th>\n      <th>2328</th>\n      <th>2329</th>\n      <th>2330</th>\n      <th>2331</th>\n      <th>2332</th>\n      <th>2333</th>\n      <th>2334</th>\n      <th>2335</th>\n      <th>2336</th>\n      <th>2337</th>\n      <th>2338</th>\n      <th>2339</th>\n      <th>2340</th>\n      <th>2341</th>\n      <th>2342</th>\n      <th>2343</th>\n      <th>2344</th>\n      <th>2345</th>\n      <th>2346</th>\n      <th>2347</th>\n      <th>2348</th>\n      <th>2349</th>\n      <th>2350</th>\n      <th>2351</th>\n      <th>2352</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.000000</td>\n      <td>0.003922</td>\n      <td>0.000000</td>\n      <td>0.074510</td>\n      <td>0.078431</td>\n      <td>0.043137</td>\n      <td>0.454902</td>\n      <td>0.462745</td>\n      <td>0.384314</td>\n      <td>0.592157</td>\n      <td>0.592157</td>\n      <td>0.474510</td>\n      <td>0.650980</td>\n      <td>0.654902</td>\n      <td>0.490196</td>\n      <td>0.517647</td>\n      <td>0.505882</td>\n      <td>0.305882</td>\n      <td>0.623529</td>\n      <td>0.600000</td>\n      <td>0.368627</td>\n      <td>0.654902</td>\n      <td>0.619608</td>\n      <td>0.372549</td>\n      <td>0.509804</td>\n      <td>0.470588</td>\n      <td>0.219608</td>\n      <td>0.513725</td>\n      <td>0.466667</td>\n      <td>0.215686</td>\n      <td>0.501961</td>\n      <td>0.454902</td>\n      <td>0.203922</td>\n      <td>0.494118</td>\n      <td>0.447059</td>\n      <td>0.188235</td>\n      <td>0.482353</td>\n      <td>0.447059</td>\n      <td>0.180392</td>\n      <td>0.486275</td>\n      <td>...</td>\n      <td>0.517647</td>\n      <td>0.501961</td>\n      <td>0.223529</td>\n      <td>0.537255</td>\n      <td>0.525490</td>\n      <td>0.258824</td>\n      <td>0.537255</td>\n      <td>0.517647</td>\n      <td>0.270588</td>\n      <td>0.564706</td>\n      <td>0.545098</td>\n      <td>0.305882</td>\n      <td>0.615686</td>\n      <td>0.592157</td>\n      <td>0.360784</td>\n      <td>0.682353</td>\n      <td>0.666667</td>\n      <td>0.435294</td>\n      <td>0.721569</td>\n      <td>0.701961</td>\n      <td>0.486275</td>\n      <td>0.705882</td>\n      <td>0.698039</td>\n      <td>0.486275</td>\n      <td>0.725490</td>\n      <td>0.709804</td>\n      <td>0.517647</td>\n      <td>0.560784</td>\n      <td>0.556863</td>\n      <td>0.384314</td>\n      <td>0.556863</td>\n      <td>0.549020</td>\n      <td>0.392157</td>\n      <td>0.584314</td>\n      <td>0.580392</td>\n      <td>0.447059</td>\n      <td>0.545098</td>\n      <td>0.537255</td>\n      <td>0.415686</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.215686</td>\n      <td>0.219608</td>\n      <td>0.235294</td>\n      <td>0.196078</td>\n      <td>0.207843</td>\n      <td>0.227451</td>\n      <td>0.211765</td>\n      <td>0.235294</td>\n      <td>0.254902</td>\n      <td>0.172549</td>\n      <td>0.203922</td>\n      <td>0.231373</td>\n      <td>0.149020</td>\n      <td>0.192157</td>\n      <td>0.207843</td>\n      <td>0.243137</td>\n      <td>0.274510</td>\n      <td>0.274510</td>\n      <td>0.494118</td>\n      <td>0.521569</td>\n      <td>0.494118</td>\n      <td>0.898039</td>\n      <td>0.898039</td>\n      <td>0.835294</td>\n      <td>0.701961</td>\n      <td>0.666667</td>\n      <td>0.560784</td>\n      <td>0.521569</td>\n      <td>0.458824</td>\n      <td>0.317647</td>\n      <td>0.607843</td>\n      <td>0.517647</td>\n      <td>0.337255</td>\n      <td>0.580392</td>\n      <td>0.474510</td>\n      <td>0.278431</td>\n      <td>0.600000</td>\n      <td>0.478431</td>\n      <td>0.270588</td>\n      <td>0.560784</td>\n      <td>...</td>\n      <td>0.847059</td>\n      <td>0.725490</td>\n      <td>0.517647</td>\n      <td>0.858824</td>\n      <td>0.764706</td>\n      <td>0.584314</td>\n      <td>0.850980</td>\n      <td>0.772549</td>\n      <td>0.611765</td>\n      <td>0.870588</td>\n      <td>0.800000</td>\n      <td>0.639216</td>\n      <td>0.921569</td>\n      <td>0.858824</td>\n      <td>0.713725</td>\n      <td>0.952941</td>\n      <td>0.905882</td>\n      <td>0.772549</td>\n      <td>0.905882</td>\n      <td>0.866667</td>\n      <td>0.749020</td>\n      <td>0.858824</td>\n      <td>0.839216</td>\n      <td>0.741176</td>\n      <td>0.898039</td>\n      <td>0.886275</td>\n      <td>0.803922</td>\n      <td>0.980392</td>\n      <td>0.980392</td>\n      <td>0.909804</td>\n      <td>0.984314</td>\n      <td>0.984314</td>\n      <td>0.929412</td>\n      <td>0.980392</td>\n      <td>1.000000</td>\n      <td>0.941176</td>\n      <td>0.988235</td>\n      <td>1.000000</td>\n      <td>0.956863</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.443137</td>\n      <td>0.466667</td>\n      <td>0.423529</td>\n      <td>0.474510</td>\n      <td>0.482353</td>\n      <td>0.435294</td>\n      <td>0.564706</td>\n      <td>0.560784</td>\n      <td>0.505882</td>\n      <td>0.627451</td>\n      <td>0.596078</td>\n      <td>0.529412</td>\n      <td>0.576471</td>\n      <td>0.529412</td>\n      <td>0.435294</td>\n      <td>0.513725</td>\n      <td>0.458824</td>\n      <td>0.349020</td>\n      <td>0.513725</td>\n      <td>0.458824</td>\n      <td>0.317647</td>\n      <td>0.513725</td>\n      <td>0.466667</td>\n      <td>0.309804</td>\n      <td>0.454902</td>\n      <td>0.435294</td>\n      <td>0.243137</td>\n      <td>0.462745</td>\n      <td>0.447059</td>\n      <td>0.239216</td>\n      <td>0.415686</td>\n      <td>0.411765</td>\n      <td>0.184314</td>\n      <td>0.411765</td>\n      <td>0.407843</td>\n      <td>0.168627</td>\n      <td>0.462745</td>\n      <td>0.454902</td>\n      <td>0.203922</td>\n      <td>0.447059</td>\n      <td>...</td>\n      <td>0.541176</td>\n      <td>0.564706</td>\n      <td>0.349020</td>\n      <td>0.588235</td>\n      <td>0.592157</td>\n      <td>0.439216</td>\n      <td>0.666667</td>\n      <td>0.658824</td>\n      <td>0.541176</td>\n      <td>0.717647</td>\n      <td>0.709804</td>\n      <td>0.588235</td>\n      <td>0.694118</td>\n      <td>0.686275</td>\n      <td>0.564706</td>\n      <td>0.607843</td>\n      <td>0.615686</td>\n      <td>0.498039</td>\n      <td>0.580392</td>\n      <td>0.584314</td>\n      <td>0.482353</td>\n      <td>0.721569</td>\n      <td>0.725490</td>\n      <td>0.647059</td>\n      <td>0.945098</td>\n      <td>0.937255</td>\n      <td>0.894118</td>\n      <td>0.937255</td>\n      <td>0.929412</td>\n      <td>0.925490</td>\n      <td>0.933333</td>\n      <td>0.917647</td>\n      <td>0.937255</td>\n      <td>0.913725</td>\n      <td>0.909804</td>\n      <td>0.949020</td>\n      <td>0.909804</td>\n      <td>0.901961</td>\n      <td>0.949020</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.784314</td>\n      <td>0.764706</td>\n      <td>0.729412</td>\n      <td>0.650980</td>\n      <td>0.619608</td>\n      <td>0.568627</td>\n      <td>0.525490</td>\n      <td>0.474510</td>\n      <td>0.388235</td>\n      <td>0.603922</td>\n      <td>0.533333</td>\n      <td>0.419608</td>\n      <td>0.619608</td>\n      <td>0.533333</td>\n      <td>0.372549</td>\n      <td>0.701961</td>\n      <td>0.600000</td>\n      <td>0.419608</td>\n      <td>0.619608</td>\n      <td>0.509804</td>\n      <td>0.325490</td>\n      <td>0.549020</td>\n      <td>0.439216</td>\n      <td>0.254902</td>\n      <td>0.509804</td>\n      <td>0.407843</td>\n      <td>0.227451</td>\n      <td>0.458824</td>\n      <td>0.364706</td>\n      <td>0.200000</td>\n      <td>0.439216</td>\n      <td>0.345098</td>\n      <td>0.203922</td>\n      <td>0.447059</td>\n      <td>0.360784</td>\n      <td>0.223529</td>\n      <td>0.423529</td>\n      <td>0.333333</td>\n      <td>0.207843</td>\n      <td>0.380392</td>\n      <td>...</td>\n      <td>0.596078</td>\n      <td>0.513725</td>\n      <td>0.364706</td>\n      <td>0.643137</td>\n      <td>0.568627</td>\n      <td>0.423529</td>\n      <td>0.682353</td>\n      <td>0.607843</td>\n      <td>0.470588</td>\n      <td>0.674510</td>\n      <td>0.603922</td>\n      <td>0.482353</td>\n      <td>0.517647</td>\n      <td>0.458824</td>\n      <td>0.356863</td>\n      <td>0.235294</td>\n      <td>0.188235</td>\n      <td>0.117647</td>\n      <td>0.243137</td>\n      <td>0.203922</td>\n      <td>0.164706</td>\n      <td>0.290196</td>\n      <td>0.266667</td>\n      <td>0.247059</td>\n      <td>0.400000</td>\n      <td>0.380392</td>\n      <td>0.384314</td>\n      <td>0.796078</td>\n      <td>0.784314</td>\n      <td>0.792157</td>\n      <td>0.976471</td>\n      <td>0.972549</td>\n      <td>0.988235</td>\n      <td>0.992157</td>\n      <td>0.996078</td>\n      <td>1.000000</td>\n      <td>0.976471</td>\n      <td>0.980392</td>\n      <td>0.996078</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.788235</td>\n      <td>0.745098</td>\n      <td>0.756863</td>\n      <td>0.784314</td>\n      <td>0.737255</td>\n      <td>0.737255</td>\n      <td>0.768627</td>\n      <td>0.705882</td>\n      <td>0.682353</td>\n      <td>0.850980</td>\n      <td>0.784314</td>\n      <td>0.733333</td>\n      <td>0.878431</td>\n      <td>0.803922</td>\n      <td>0.721569</td>\n      <td>0.862745</td>\n      <td>0.788235</td>\n      <td>0.682353</td>\n      <td>0.803922</td>\n      <td>0.733333</td>\n      <td>0.611765</td>\n      <td>0.549020</td>\n      <td>0.482353</td>\n      <td>0.352941</td>\n      <td>0.231373</td>\n      <td>0.184314</td>\n      <td>0.050980</td>\n      <td>0.235294</td>\n      <td>0.188235</td>\n      <td>0.054902</td>\n      <td>0.254902</td>\n      <td>0.203922</td>\n      <td>0.054902</td>\n      <td>0.286275</td>\n      <td>0.219608</td>\n      <td>0.066667</td>\n      <td>0.313725</td>\n      <td>0.239216</td>\n      <td>0.062745</td>\n      <td>0.368627</td>\n      <td>...</td>\n      <td>0.635294</td>\n      <td>0.564706</td>\n      <td>0.317647</td>\n      <td>0.733333</td>\n      <td>0.666667</td>\n      <td>0.443137</td>\n      <td>0.827451</td>\n      <td>0.756863</td>\n      <td>0.549020</td>\n      <td>0.870588</td>\n      <td>0.803922</td>\n      <td>0.611765</td>\n      <td>0.862745</td>\n      <td>0.788235</td>\n      <td>0.619608</td>\n      <td>0.898039</td>\n      <td>0.831373</td>\n      <td>0.678431</td>\n      <td>0.984314</td>\n      <td>0.905882</td>\n      <td>0.784314</td>\n      <td>0.964706</td>\n      <td>0.898039</td>\n      <td>0.796078</td>\n      <td>0.878431</td>\n      <td>0.811765</td>\n      <td>0.737255</td>\n      <td>0.878431</td>\n      <td>0.811765</td>\n      <td>0.760784</td>\n      <td>0.941176</td>\n      <td>0.882353</td>\n      <td>0.847059</td>\n      <td>0.905882</td>\n      <td>0.858824</td>\n      <td>0.819608</td>\n      <td>0.917647</td>\n      <td>0.874510</td>\n      <td>0.843137</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "ef362e8c227b50ea1d6a20695c38447f89fa0350",
        "trusted": true
      },
      "cell_type": "code",
      "source": "#preparing test data\nfin = pd.read_csv('../input/test.csv')\nfin = fin.drop(columns=[\"Unnamed: 0\"])\nfin.head()",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 33,
          "data": {
            "text/plain": "          0         1         2  ...       2350      2351  2352\n0  0.000000  0.003922  0.000000  ...   0.537255  0.415686     0\n1  0.443137  0.466667  0.423529  ...   0.901961  0.949020     0\n2  0.647059  0.682353  0.760784  ...   0.949020  0.952941     6\n3  0.658824  0.572549  0.431373  ...   0.290196  0.200000     2\n4  0.937255  0.901961  0.866667  ...   1.000000  0.984314     0\n\n[5 rows x 2353 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n      <th>12</th>\n      <th>13</th>\n      <th>14</th>\n      <th>15</th>\n      <th>16</th>\n      <th>17</th>\n      <th>18</th>\n      <th>19</th>\n      <th>20</th>\n      <th>21</th>\n      <th>22</th>\n      <th>23</th>\n      <th>24</th>\n      <th>25</th>\n      <th>26</th>\n      <th>27</th>\n      <th>28</th>\n      <th>29</th>\n      <th>30</th>\n      <th>31</th>\n      <th>32</th>\n      <th>33</th>\n      <th>34</th>\n      <th>35</th>\n      <th>36</th>\n      <th>37</th>\n      <th>38</th>\n      <th>39</th>\n      <th>...</th>\n      <th>2313</th>\n      <th>2314</th>\n      <th>2315</th>\n      <th>2316</th>\n      <th>2317</th>\n      <th>2318</th>\n      <th>2319</th>\n      <th>2320</th>\n      <th>2321</th>\n      <th>2322</th>\n      <th>2323</th>\n      <th>2324</th>\n      <th>2325</th>\n      <th>2326</th>\n      <th>2327</th>\n      <th>2328</th>\n      <th>2329</th>\n      <th>2330</th>\n      <th>2331</th>\n      <th>2332</th>\n      <th>2333</th>\n      <th>2334</th>\n      <th>2335</th>\n      <th>2336</th>\n      <th>2337</th>\n      <th>2338</th>\n      <th>2339</th>\n      <th>2340</th>\n      <th>2341</th>\n      <th>2342</th>\n      <th>2343</th>\n      <th>2344</th>\n      <th>2345</th>\n      <th>2346</th>\n      <th>2347</th>\n      <th>2348</th>\n      <th>2349</th>\n      <th>2350</th>\n      <th>2351</th>\n      <th>2352</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.000000</td>\n      <td>0.003922</td>\n      <td>0.000000</td>\n      <td>0.074510</td>\n      <td>0.078431</td>\n      <td>0.043137</td>\n      <td>0.454902</td>\n      <td>0.462745</td>\n      <td>0.384314</td>\n      <td>0.592157</td>\n      <td>0.592157</td>\n      <td>0.474510</td>\n      <td>0.650980</td>\n      <td>0.654902</td>\n      <td>0.490196</td>\n      <td>0.517647</td>\n      <td>0.505882</td>\n      <td>0.305882</td>\n      <td>0.623529</td>\n      <td>0.600000</td>\n      <td>0.368627</td>\n      <td>0.654902</td>\n      <td>0.619608</td>\n      <td>0.372549</td>\n      <td>0.509804</td>\n      <td>0.470588</td>\n      <td>0.219608</td>\n      <td>0.513725</td>\n      <td>0.466667</td>\n      <td>0.215686</td>\n      <td>0.501961</td>\n      <td>0.454902</td>\n      <td>0.203922</td>\n      <td>0.494118</td>\n      <td>0.447059</td>\n      <td>0.188235</td>\n      <td>0.482353</td>\n      <td>0.447059</td>\n      <td>0.180392</td>\n      <td>0.486275</td>\n      <td>...</td>\n      <td>0.517647</td>\n      <td>0.501961</td>\n      <td>0.223529</td>\n      <td>0.537255</td>\n      <td>0.525490</td>\n      <td>0.258824</td>\n      <td>0.537255</td>\n      <td>0.517647</td>\n      <td>0.270588</td>\n      <td>0.564706</td>\n      <td>0.545098</td>\n      <td>0.305882</td>\n      <td>0.615686</td>\n      <td>0.592157</td>\n      <td>0.360784</td>\n      <td>0.682353</td>\n      <td>0.666667</td>\n      <td>0.435294</td>\n      <td>0.721569</td>\n      <td>0.701961</td>\n      <td>0.486275</td>\n      <td>0.705882</td>\n      <td>0.698039</td>\n      <td>0.486275</td>\n      <td>0.725490</td>\n      <td>0.709804</td>\n      <td>0.517647</td>\n      <td>0.560784</td>\n      <td>0.556863</td>\n      <td>0.384314</td>\n      <td>0.556863</td>\n      <td>0.549020</td>\n      <td>0.392157</td>\n      <td>0.584314</td>\n      <td>0.580392</td>\n      <td>0.447059</td>\n      <td>0.545098</td>\n      <td>0.537255</td>\n      <td>0.415686</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.443137</td>\n      <td>0.466667</td>\n      <td>0.423529</td>\n      <td>0.474510</td>\n      <td>0.482353</td>\n      <td>0.435294</td>\n      <td>0.564706</td>\n      <td>0.560784</td>\n      <td>0.505882</td>\n      <td>0.627451</td>\n      <td>0.596078</td>\n      <td>0.529412</td>\n      <td>0.576471</td>\n      <td>0.529412</td>\n      <td>0.435294</td>\n      <td>0.513725</td>\n      <td>0.458824</td>\n      <td>0.349020</td>\n      <td>0.513725</td>\n      <td>0.458824</td>\n      <td>0.317647</td>\n      <td>0.513725</td>\n      <td>0.466667</td>\n      <td>0.309804</td>\n      <td>0.454902</td>\n      <td>0.435294</td>\n      <td>0.243137</td>\n      <td>0.462745</td>\n      <td>0.447059</td>\n      <td>0.239216</td>\n      <td>0.415686</td>\n      <td>0.411765</td>\n      <td>0.184314</td>\n      <td>0.411765</td>\n      <td>0.407843</td>\n      <td>0.168627</td>\n      <td>0.462745</td>\n      <td>0.454902</td>\n      <td>0.203922</td>\n      <td>0.447059</td>\n      <td>...</td>\n      <td>0.541176</td>\n      <td>0.564706</td>\n      <td>0.349020</td>\n      <td>0.588235</td>\n      <td>0.592157</td>\n      <td>0.439216</td>\n      <td>0.666667</td>\n      <td>0.658824</td>\n      <td>0.541176</td>\n      <td>0.717647</td>\n      <td>0.709804</td>\n      <td>0.588235</td>\n      <td>0.694118</td>\n      <td>0.686275</td>\n      <td>0.564706</td>\n      <td>0.607843</td>\n      <td>0.615686</td>\n      <td>0.498039</td>\n      <td>0.580392</td>\n      <td>0.584314</td>\n      <td>0.482353</td>\n      <td>0.721569</td>\n      <td>0.725490</td>\n      <td>0.647059</td>\n      <td>0.945098</td>\n      <td>0.937255</td>\n      <td>0.894118</td>\n      <td>0.937255</td>\n      <td>0.929412</td>\n      <td>0.925490</td>\n      <td>0.933333</td>\n      <td>0.917647</td>\n      <td>0.937255</td>\n      <td>0.913725</td>\n      <td>0.909804</td>\n      <td>0.949020</td>\n      <td>0.909804</td>\n      <td>0.901961</td>\n      <td>0.949020</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.647059</td>\n      <td>0.682353</td>\n      <td>0.760784</td>\n      <td>0.811765</td>\n      <td>0.847059</td>\n      <td>0.925490</td>\n      <td>0.439216</td>\n      <td>0.466667</td>\n      <td>0.541176</td>\n      <td>0.690196</td>\n      <td>0.717647</td>\n      <td>0.784314</td>\n      <td>0.839216</td>\n      <td>0.866667</td>\n      <td>0.925490</td>\n      <td>0.901961</td>\n      <td>0.917647</td>\n      <td>0.960784</td>\n      <td>0.905882</td>\n      <td>0.917647</td>\n      <td>0.949020</td>\n      <td>0.615686</td>\n      <td>0.619608</td>\n      <td>0.635294</td>\n      <td>0.392157</td>\n      <td>0.372549</td>\n      <td>0.376471</td>\n      <td>0.329412</td>\n      <td>0.294118</td>\n      <td>0.282353</td>\n      <td>0.341176</td>\n      <td>0.294118</td>\n      <td>0.278431</td>\n      <td>0.368627</td>\n      <td>0.298039</td>\n      <td>0.270588</td>\n      <td>0.325490</td>\n      <td>0.235294</td>\n      <td>0.203922</td>\n      <td>0.313725</td>\n      <td>...</td>\n      <td>0.498039</td>\n      <td>0.517647</td>\n      <td>0.403922</td>\n      <td>0.501961</td>\n      <td>0.505882</td>\n      <td>0.372549</td>\n      <td>0.439216</td>\n      <td>0.435294</td>\n      <td>0.301961</td>\n      <td>0.447059</td>\n      <td>0.450980</td>\n      <td>0.317647</td>\n      <td>0.490196</td>\n      <td>0.498039</td>\n      <td>0.380392</td>\n      <td>0.650980</td>\n      <td>0.662745</td>\n      <td>0.564706</td>\n      <td>0.847059</td>\n      <td>0.866667</td>\n      <td>0.784314</td>\n      <td>0.878431</td>\n      <td>0.898039</td>\n      <td>0.839216</td>\n      <td>0.862745</td>\n      <td>0.886275</td>\n      <td>0.843137</td>\n      <td>0.886275</td>\n      <td>0.917647</td>\n      <td>0.890196</td>\n      <td>0.917647</td>\n      <td>0.945098</td>\n      <td>0.933333</td>\n      <td>0.937255</td>\n      <td>0.960784</td>\n      <td>0.956863</td>\n      <td>0.929412</td>\n      <td>0.949020</td>\n      <td>0.952941</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.658824</td>\n      <td>0.572549</td>\n      <td>0.431373</td>\n      <td>0.647059</td>\n      <td>0.560784</td>\n      <td>0.419608</td>\n      <td>0.615686</td>\n      <td>0.541176</td>\n      <td>0.403922</td>\n      <td>0.588235</td>\n      <td>0.513725</td>\n      <td>0.376471</td>\n      <td>0.529412</td>\n      <td>0.470588</td>\n      <td>0.341176</td>\n      <td>0.486275</td>\n      <td>0.427451</td>\n      <td>0.298039</td>\n      <td>0.470588</td>\n      <td>0.419608</td>\n      <td>0.294118</td>\n      <td>0.486275</td>\n      <td>0.435294</td>\n      <td>0.309804</td>\n      <td>0.454902</td>\n      <td>0.415686</td>\n      <td>0.282353</td>\n      <td>0.447059</td>\n      <td>0.407843</td>\n      <td>0.274510</td>\n      <td>0.443137</td>\n      <td>0.400000</td>\n      <td>0.250980</td>\n      <td>0.431373</td>\n      <td>0.388235</td>\n      <td>0.239216</td>\n      <td>0.435294</td>\n      <td>0.388235</td>\n      <td>0.223529</td>\n      <td>0.431373</td>\n      <td>...</td>\n      <td>0.345098</td>\n      <td>0.290196</td>\n      <td>0.176471</td>\n      <td>0.301961</td>\n      <td>0.254902</td>\n      <td>0.160784</td>\n      <td>0.294118</td>\n      <td>0.243137</td>\n      <td>0.156863</td>\n      <td>0.290196</td>\n      <td>0.247059</td>\n      <td>0.160784</td>\n      <td>0.278431</td>\n      <td>0.235294</td>\n      <td>0.149020</td>\n      <td>0.207843</td>\n      <td>0.184314</td>\n      <td>0.094118</td>\n      <td>0.333333</td>\n      <td>0.309804</td>\n      <td>0.219608</td>\n      <td>0.309804</td>\n      <td>0.290196</td>\n      <td>0.207843</td>\n      <td>0.317647</td>\n      <td>0.298039</td>\n      <td>0.215686</td>\n      <td>0.356863</td>\n      <td>0.337255</td>\n      <td>0.254902</td>\n      <td>0.380392</td>\n      <td>0.360784</td>\n      <td>0.278431</td>\n      <td>0.352941</td>\n      <td>0.329412</td>\n      <td>0.239216</td>\n      <td>0.313725</td>\n      <td>0.290196</td>\n      <td>0.200000</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.937255</td>\n      <td>0.901961</td>\n      <td>0.866667</td>\n      <td>0.870588</td>\n      <td>0.831373</td>\n      <td>0.792157</td>\n      <td>0.898039</td>\n      <td>0.850980</td>\n      <td>0.803922</td>\n      <td>0.615686</td>\n      <td>0.556863</td>\n      <td>0.494118</td>\n      <td>0.352941</td>\n      <td>0.286275</td>\n      <td>0.211765</td>\n      <td>0.364706</td>\n      <td>0.282353</td>\n      <td>0.196078</td>\n      <td>0.325490</td>\n      <td>0.231373</td>\n      <td>0.121569</td>\n      <td>0.345098</td>\n      <td>0.235294</td>\n      <td>0.113725</td>\n      <td>0.333333</td>\n      <td>0.231373</td>\n      <td>0.086275</td>\n      <td>0.345098</td>\n      <td>0.247059</td>\n      <td>0.082353</td>\n      <td>0.337255</td>\n      <td>0.247059</td>\n      <td>0.066667</td>\n      <td>0.329412</td>\n      <td>0.247059</td>\n      <td>0.047059</td>\n      <td>0.333333</td>\n      <td>0.270588</td>\n      <td>0.062745</td>\n      <td>0.364706</td>\n      <td>...</td>\n      <td>0.917647</td>\n      <td>0.858824</td>\n      <td>0.678431</td>\n      <td>0.901961</td>\n      <td>0.839216</td>\n      <td>0.658824</td>\n      <td>0.921569</td>\n      <td>0.858824</td>\n      <td>0.690196</td>\n      <td>0.945098</td>\n      <td>0.890196</td>\n      <td>0.725490</td>\n      <td>0.945098</td>\n      <td>0.901961</td>\n      <td>0.752941</td>\n      <td>0.921569</td>\n      <td>0.886275</td>\n      <td>0.752941</td>\n      <td>0.862745</td>\n      <td>0.839216</td>\n      <td>0.725490</td>\n      <td>0.796078</td>\n      <td>0.792157</td>\n      <td>0.690196</td>\n      <td>0.749020</td>\n      <td>0.752941</td>\n      <td>0.674510</td>\n      <td>0.819608</td>\n      <td>0.831373</td>\n      <td>0.768627</td>\n      <td>0.909804</td>\n      <td>0.937255</td>\n      <td>0.886275</td>\n      <td>0.984314</td>\n      <td>1.000000</td>\n      <td>0.976471</td>\n      <td>0.980392</td>\n      <td>1.000000</td>\n      <td>0.984314</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "e334cf9128a80a96bad3bad3a7e8cb4c60c608e4",
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Train/ Fit the model on train data\nY = final[\"2352\"]\nX = final.drop(columns=[\"2352\"])\nY = tf.keras.utils.to_categorical(Y,10)\nX = np.array(X)\nX = X.reshape(X.shape[0],28,28,3)\nY_v = fin[\"2352\"]\nX_v = fin.drop(columns=[\"2352\"])\nY_v = tf.keras.utils.to_categorical(Y_v,10)\nX_v = np.array(X_v)\nX_v = X_v.reshape(9778,28,28,3)\n",
      "execution_count": 34,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f6ffa0a4b49ba89203ec674a83065f13d3aaa5d9"
      },
      "cell_type": "code",
      "source": "model.fit(X,Y,epochs =15)",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Epoch 1/15\n23705/23705 [==============================] - 15s 648us/step - loss: 0.4290 - acc: 0.8802\nEpoch 2/15\n23705/23705 [==============================] - 16s 687us/step - loss: 0.4219 - acc: 0.8836\nEpoch 3/15\n23705/23705 [==============================] - 16s 655us/step - loss: 0.4174 - acc: 0.8853\nEpoch 4/15\n23705/23705 [==============================] - 15s 648us/step - loss: 0.4138 - acc: 0.8867\nEpoch 5/15\n23705/23705 [==============================] - 16s 685us/step - loss: 0.4054 - acc: 0.8892\nEpoch 6/15\n23705/23705 [==============================] - 16s 657us/step - loss: 0.3984 - acc: 0.8923\nEpoch 7/15\n23705/23705 [==============================] - 15s 650us/step - loss: 0.3901 - acc: 0.8982\nEpoch 8/15\n23705/23705 [==============================] - 16s 686us/step - loss: 0.3884 - acc: 0.8990\nEpoch 9/15\n23705/23705 [==============================] - 16s 655us/step - loss: 0.3848 - acc: 0.8968\nEpoch 10/15\n23705/23705 [==============================] - 15s 649us/step - loss: 0.3811 - acc: 0.9019\nEpoch 11/15\n23705/23705 [==============================] - 16s 680us/step - loss: 0.3766 - acc: 0.9031\nEpoch 12/15\n23705/23705 [==============================] - 16s 668us/step - loss: 0.3681 - acc: 0.9072\nEpoch 13/15\n23705/23705 [==============================] - 16s 655us/step - loss: 0.3740 - acc: 0.9071\nEpoch 14/15\n23705/23705 [==============================] - 16s 684us/step - loss: 0.3633 - acc: 0.9101\nEpoch 15/15\n23705/23705 [==============================] - 16s 660us/step - loss: 0.3680 - acc: 0.9088\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 39,
          "data": {
            "text/plain": "<keras.callbacks.History at 0x7fb66ecd9438>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "8da0b27de97382ef2d25fb415ed9a84adf23e63f",
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Evaluating on test data\nmodel.save('weights.h5')\npred= model.evaluate(X_v,Y_v)\nprint(\"On test data:\")\nprint('Loss: ',pred[0])\nacc = pred[1]*100\nprint('Accuracy: ',acc,\"%\")",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": "9778/9778 [==============================] - 2s 193us/step\nOn test data:\nLoss:  0.24733946012631347\nAccuracy:  95.89895683823248 %\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "042d57f7acd7784e6cb4e4c219927e4f13e2a137"
      },
      "cell_type": "code",
      "source": "#Hurray!",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}